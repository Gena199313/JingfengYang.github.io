---
layout: default 
---
<header class="bloghead">
    <h1 class="bloghead-title">
    <a href="{{ site.url }}/">{{ site.name }}<span>&#39;s homepage</span></a>
  </h1>
    <nav class="bloghead-nav">
        {% for nav in site.nav %}
        <a href="{{ nav.href }}">{{ nav.name }}<span> &nbsp;/&nbsp; </span></a> {% endfor %}
    </nav>
</header>
<div class="content">
    <a name="bio"></a>
    <div class="post-list">
        <h2 class="post-title">
        Bio
    	</h2> 
        <p>I am an Applied Research Scientist at <a href="https://amazonsearchqu.github.io/" style="color:#4133ff;">Amazon Search/A9 QU</a>. I dropped out of <a href="https://www.cs.washington.edu/research/nlp/people" style="color:#4133ff;">UW CS PhD program (NLP track)</a> in 2021 due to some personal reasons (UW is definitely one of the best places to do NLP research). I received M.S. in Computer Science (Machine Learning Specialization) at <a href="https://www.gatech.edu/" style="color:#4133ff;">Georgia Tech</a> in 2021, where I worked with Prof. <a href="https://cs.stanford.edu/~diyiy/" style="color:#4133ff;">Diyi Yang</a> at SALT lab, currently at <a href="https://nlp.stanford.edu/people/" style="color:#4133ff;">Stanford University</a>. Before that,  I received B.S. in Biological Science and Computer Science at <a href="http://english.pku.edu.cn/" style="color:#4133ff;">Peking University</a> in 2019, where I worked with Prof. <a href="https://pku-tangent.github.io/" style="color:#4133ff;">Sujian Li</a>. In summer 2018, I worked as a research intern with Prof. <a href="https://www.aclweb.org/portal/content/bonnie-webber-receives-2020-acl-life-time-achievement-award" style="color:#4133ff;">Bonnie Webber</a> at <a href="https://www.ed.ac.uk/" style="color:#4133ff;">the University of Edinburgh</a>. I also researched as an intern at <a href="https://en.wikipedia.org/wiki/A9.com" style="color:#4133ff;">Amazon Search/A9</a> (Fall 2021), <a href="https://research.google/" style="color:#4133ff;">Google</a> (Summer 2021) and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" style="color:#4133ff;">Microsoft</a> (Spring 2019), and conducted software development while interning at <a href="https://www.amazon.jobs/en/location/san-francisco-bay-area-ca" style="color:#4133ff;">Amazon</a> (Summer 2020). </p>
            
        <p> I have a broad interest in Natural Language Processing <a href="https://zhuanlan.zhihu.com/p/539706909" style="color:#4133ff;">[Blog: NLP Trends from the perspective of LLM based on ACL 2022 (Chinese)]</a>  and Machine Learning. My research goal is to 1) solve the data scarcity problem, 2) improve the generalizability of models, and 3) design controllable and interpretable models,
            by 1) using unlabeled, out-of-domain or augmented data, 2) incorporating external knowledge or inductive biases (e.g. intermediate abstractions, model architecture biases etc.) into models, and 3) large-scale pretraining/LLMs <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog: Reproduction and Usage of GPT3/ChatGPT]</a>.</p>

        <p> Previously, I was trying to achieve this goal in 1) text generation, 2) semantic parsing / question answering, 3) multilingual NLP, and 4) other Natural Languge/Multimoldality understanding tasks.

        <p> Recent advances in the field always make me rethink the utility of inductive biases in the future, considering that the effect of structral biases is decreasing with larger pretrained unified multimodal, multitask, multilingual Transformer models [<a href="https://github.com/JingfengYang/Multi-modal-Deep-Learning" style="color:#4133ff;">Repo: Paper List of Multi-modal Deep Learning</a>]. 

        <p> I studied a lot about life science when I was an undergraduate, thus I'm also excited about any opportunities of leveraging AI to advance science (especially biological science and healthcare).    

        <p> I am open to any type of coffee chat. If you are interested in talking with me, feel free to send me an email.

    </div>
    <a name="news"></a>
    <div class="post-list">
        <h2 class="post-title">
        News
        </h2> 
        <p><b>Feb 13th, 2023:</b> New Blog on Reproduction and Usage of GPT3/ChatGPT. <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog]</a></p> 
        <p><b>Nov 17th, 2022:</b> Gave a talk on Table Understanding and Text2SQL in Alibaba DAMO Academy. <a href="resources/slides/Table_Understanding_and_Text2SQL.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Aug 4th, 2022:</b> Gave a talk on Compositional Generalization in <a href="https://event.baai.ac.cn/activities/489" style="color:#4133ff;">BAAI Seminar</a>. <a href="resources/slides/Compositional_Generalization.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Jun 10th, 2022:</b> Gave a talk on TableFormer in A9 ML Talk Series. <a href="resources/slides/Amazon_TableFormer.pdf" style="color:#4133ff;">[Slides]</a></p>
        <p><b>Apr 7th, 2022:</b> Our paper "SUBS: Subtree Substitution for Compositional Semantic Parsing." is accepted at NAACL 2022. <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog: Compositional Generalization (Chinese)]</a> </p>
        <p><b>Apr 7th, 2022:</b> Our paper "Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models." is accepted at NAACL 2022 Findings. </p>
        <p><b>Feb 23th, 2022:</b> Our paper "TableFormer: Robust Transformer Modeling for Table-Text Encoding." is accepted at ACL 2022 (<font color="red">Oral</font>). <a href="https://mp.weixin.qq.com/s/ikuHpGt3nYYkitUYlesdfA" style="color:#4133ff;">[Blog (Chinese)]</a> <a href="https://github.com/google-research/tapas/blob/master/TABLEFORMER.md" style="color:red">[Code Released]</a>
        <p><b>Jan 17th, 2022:</b> I returned to Amazon Search as an applied scientist. </p>
        <p><b>Sep 27th, 2021:</b> I started my applied scientist internship at Amazon. </p>
        <p><b>Aug 26th, 2021:</b> Our paper "Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing." is accepted at EMNLP 2021. </p>
        <p><b>Aug 26th, 2021:</b> Our paper "WIKIBIAS: Detecting Multi-Span Subjective Biases in Language." is accepted at EMNLP 2021 Findings. </p>
        <p><b>Jul 12th, 2021:</b> I started my research internship at Google. </p>
        <p><b>Mar 28th, 2021:</b> Gave some lectures for <a href="https://sites.google.com/view/nlp-bootcamp-s21/home" style="color:#4133ff;">NLP bootcamp</a> as a TA. </p>
        <p><b>Feb 15th, 2021:</b> Gave a lecture on PyTorch as head TA in <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/" style="color:#4133ff;">CS-4650 Natural Language Processing.</a> <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/slides/Introduction_to_PyTorch.pdf" style="color:#4133ff;">[Slides]</a></p>
        <p><b>Sep 15th, 2020:</b> Our paper "Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection." is accepted at EMNLP 2020. </p>
        <p><b>Sep 2nd, 2020:</b> Gave a lecture on Deep Learning as a TA in <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing.</a> <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/slides/Introduction_to_NN.pdf" style="color:#4133ff;">[Slides]</a></p>
    </div>
    <a name="education"></a>
    <div class="post-list">
        <h2 class="post-title">
        Education
    	</h2> 
        <p>Master of Science: Computer Science, Georgia Tech, Atlanta, America.  Aug 2019 - May 2021.</p>
        <p>Bachelor of Science: Computer Science and Biological Science, Peking University, Beijing, China.  Sep 2015 - July 2019.</p>
    </div>
    <a name="publications"></a>
    <div class="post-list">
        <h2 class="post-title">
        Publications 
        </h2> 
        <h3 class="post-title">
            Selected Work
        </h3> 
        <ul>
            <li><b>Jingfeng Yang</b>. 2023. <b>Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?</b> Blog Post, Feb 2023. <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog]</a> <br/><br/> -- Tried to answer two questions regarding reproduction and usage of GPT-3/ChatGPT. </li> <br/><br/> 
            <li><b>Jingfeng Yang</b>, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, Diyi Yang. 2022. <b>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models.</b> In NAACL' 2022 (Findings). <a href="https://arxiv.org/pdf/2205.07381.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/amzn/SeqZero" style="color:#4133ff;">[Code]</a> <a href="resources/slides/SeqZero_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog (Chinese)]</a> <br/><br/> -- <a href="https://openreview.net/forum?id=ZB58jLvryqw"  style="color:#4133ff;">The first one</a> to propose Problem Decomposition and Sequential Prompting PLMs for better compositional generalization, even before Chain-of-Thought Prompting and Least-to-Most Prompting, although CoT and Least-to-Most prompting could achieve better performance with larger LMs in a wider range of tasks :). </li> <br/><br/> 
            <li><b>Jingfeng Yang</b>, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, Shachi Paul. 2022. <b>TableFormer: Robust Transformer Modeling for Table-Text Encoding.</b> In ACL' 2022 (<font color="red">Oral</font>). <a href="https://arxiv.org/pdf/2203.00274.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/google-research/tapas/blob/master/TABLEFORMER.md" style="color:#4133ff;">[Code]</a> <a href="resources/slides/TableFormer_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/ikuHpGt3nYYkitUYlesdfA" style="color:#4133ff;">[Blog (Chinese)]</a> <br/><br/> -- The first one to identify the vulnerability to row/column perturbations for prior table-text encoding models. Designed and pretrained a TableFormer model, where learnable attention biases could help achieve strict robustness.</li>  
        </ul>
        <h3 class="post-title">
            Other Publications and Preprints
        </h3> 
        <ul>
            <li>Caleb Ziems&#42;, William Held&#42;, <b>Jingfeng Yang</b>, Diyi Yang. 2022. <b>Multi-VALUE: A Framework for Cross-Dialectal English NLP.</b> Preprint 2022. <a href="https://arxiv.org/pdf/2212.08011.pdf" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Xutan Peng, Yipeng Zhang, <b>Jingfeng Yang</b>, Mark Stevenson. 2022. <b>On the Security Vulnerabilities of Text-to-SQL Models.</b> Preprint 2022. <a href="https://arxiv.org/pdf/2211.15363.pdf" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Ruijie Wang, Zheng Li, <b>Jingfeng Yang</b>, Tianyu Cao, Chao Zhang, Bing Yin, Tarek Abdelzaher. 2023. <b>Mutually-paced Knowledge Distillation for Cross-lingual Temporal Knowledge Graph Reasoning.</b> In WWW' 2023. <a href="" style="color:#4133ff;">[Paper]</a></li>
            <li><b>Jingfeng Yang</b>&#42;, Le Zhang&#42;, Diyi Yang. 2022. <b>SUBS: Subtree Substitution for Compositional Semantic Parsing.</b> In NAACL' 2022. <a href="https://arxiv.org/pdf/2205.01538.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/GT-SALT/SUBS" style="color:#4133ff;">[Code]</a> <a href="resources/slides/SUBS_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog (Chinese)]</a> </li>
            <li><b>Jingfeng Yang</b>, Federico Fancellu, Bonnie Webber, Diyi Yang. 2021. <b>Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing.</b> In EMNLP' 2021. <a href="https://aclanthology.org/2021.emnlp-main.472.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/JingfengYang/Multilingual-DRS-Semantic-parsing" style="color:#4133ff;">[Code]</a> <a href="resources/slides/xSP_slides.pdf" style="color:#4133ff;">[Slides]</a> </li>
            <li>Yang Zhong, <b>Jingfeng Yang</b>, Wei Xu, Diyi Yang. 2021. <b>WIKIBIAS: Detecting Multi-Span Subjective Biases in Language.</b> In EMNLP' 2021 (Findings). <a href="https://aclanthology.org/2021.findings-emnlp.155.pdf" style="color:#4133ff;">[Paper]</a> </li>
            <li><b>Jingfeng Yang</b>, Zhaoran Ma, Diyi Yang. 2020. <b>Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection.</b> In EMNLP' 2020. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.113/" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/GT-SALT/Disfluency-Generation-and-Detection" style="color:#4133ff;">[Code]</a> <a href="resources/slides/disfluency_slides.pdf" style="color:#4133ff;">[Slides]</a> </li>
            <li><b>Jingfeng Yang</b>, Sujian Li. 2018. <b>Chinese Discourse Segmentation Using Bilingual Discourse Commonality.</b> Preprint. <a href="https://arxiv.org/abs/1809.01497" style="color:#4133ff;">[Paper]</a> </li>
	        <li>Yizhong Wang, Sujian Li, <b>Jingfeng Yang</b>. 2018. <b>Toward Fast and Accurate Neural Discourse Segmentation.</b> EMNLP' 2018. <a href="https://www.aclweb.org/anthology/D18-1116" style="color:#4133ff;">[Paper]</a></li>
	        <li>Yizhong Wang, Sujian Li, <b>Jingfeng Yang</b>, Xu Sun, Houfeng Wang. 2017. <b>Tag-enhanced Tree-structured Neural Networks for Implicit Discourse Relation Classification.</b> In The 8th International Joint Conference on Natural Language Processing (IJCNLP' 2017). <a href="https://arxiv.org/abs/1803.01165" style="color:#4133ff;">[Paper]</a></li>

        </ul>
        </div>

    <a name="service"></a>
    <div class="post-list">
            <h2 class="post-title">
            Service
            </h2> 
            <p>Reviewer/PC member in NLP Conferences: ACL Rolling Review, ACL' 2023, EMNLP' 2022, NAACL' 2021</p>
            <p>Reviewer/PC member in ML Conferences: NeurIPS' 2022, ICML' 2023</p>
            <p>Reviewer/PC member in AI Conferences: AAAI' 2023, IJCAI' 2023</p>
            <p>Reviewer/PC member in Data Mining Conferences/Transactions: KDD' 2023, TKDE' 2022</p>
    </div>
    <a name="research"></a>
    <div class="post-list">
        <h2 class="post-title">
        Research Experiences
    	</h2> 
	<p>Research Assistant in College of Computing, Georgia Institute of Technology. Advisor: <a href="https://cs.stanford.edu/~diyiy/" style="color:#4133ff;">Diyi Yang</a>. Aug 2019 - May 2021. </p>
    <p>Visiting Researcher in Institute for Language, Cognition and Computation, The University of Edinburgh. Advisor: <a href="http://homepages.inf.ed.ac.uk/bonnie/" style="color:#4133ff;">Bonnie Webber</a>. July 2018 - Sep 2018. </p>
	<p>Research Assistant in Department of Computational Linguistics, Peking University. Advisor: <a href="https://pku-tangent.github.io/" style="color:#4133ff;">Sujian Li</a>. July 2017 - June 2019.</p>
    </div>
    <a name="work"></a>
    <div class="post-list">
        <h2 class="post-title">
        Industry Experiences
        </h2> 
        <p>Applied Scientist, Amazon, Palo Alto. Jan 2022 - present. </p>
        <p>Applied Scientist Intern, Amazon, Palo Alto (Virtual). Mentors: <a href="https://hmjianggatech.github.io/" style="color:#4133ff;">Haoming Jiang</a>, <a href="https://danqingz.github.io/" style="color:#4133ff;">Danqing Zhang</a>, <a href="https://scholar.google.com/citations?user=P-mBKNYAAAAJ&hl=zh-CN" style="color:#4133ff;">Qingyu Yin</a>. Sep 2021 - Dec 2021. </p>
        <p>Research Intern, Google, Mountain View (Virtual). Collaborators: <a href="https://research.google/people/AdityaGupta/" style="color:#4133ff;">Aditya Gupta</a>,  <a href="https://scholar.google.com/citations?user=i7lw4LwAAAAJ&hl=en" style="color:#4133ff;">Shyam Upadhyay</a>, <a href="https://scholar.google.com/citations?user=XVVvabUAAAAJ&hl=en" style="color:#4133ff;">Luheng He</a>, <a href="https://scholar.google.com/citations?user=e6uk9EAAAAAJ&hl=en" style="color:#4133ff;">Rahul Goel</a>, <a href="https://scholar.google.com/citations?user=a_95_VkAAAAJ&hl=en" style="color:#4133ff;">Shachi Paul</a>. July 2021 - Sep 2021.</p>
        <p>Software Development Engineer Intern, Amazon, San Francisco (Virtual). May 2020 - July 2020. </p>
        <p>Research Intern, Microsoft Research Asia, Beijing, China. Mentor: <a href="https://aclanthology.org/people/j/jin-ge-yao/" style="color:#4133ff;">Jin-ge Yao</a>. December 2018 - March 2019. </p>
    </div>
    <a name="teaching"></a>
    <div class="post-list">
        <h2 class="post-title">
        Teaching Experiences
        </h2> 
        <p>Teaching Assitant for Coding Support, POLS-585 Text as Data, Emory University, Atlanta. June 2021 - July 2021. </p>
        <p>Head Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/" style="color:#4133ff;">CS-4650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Spring 2021. </p>
        <p>Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Fall 2020. </p>
	    <p>Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2020/cs7650_spring/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Spring 2020. </p>
    </div>
    
    <a name="awards"></a>
    <div class="post-list">
        <h2 class="post-title">
        Awards 
    	</h2> 
        <p>May 4th Fellowship, 2016-2017.</p>
	    <p>Kwang-Hua Fellowship, 2015-2016.</p>
	    <p>Merit Student of Peking University, 2016-2017, 2015-2016.</p>
	    <p>Silver medalist in Chinese Mathematics Olympiad (CMO), 2015.</p>
	
    </div>

    <a name="intern"></a>
    <div class="post-list">
        <h2 class="post-title">
        Intern Mentorship 
    	</h2> 
        <p><a href="http://www.zhimengjiang.com/" style="color:#4133ff;">Zhimeng Jiang</a>, Texas A&M University. Fall 2022.</p>
        <p><a href="https://www.cs.toronto.edu/~zining/" style="color:#4133ff;">Zining Zhu</a>, University of Toronto. Summer 2022.</p>
        <p><a href="https://wjerry5.github.io/" style="color:#4133ff;">Ruijie Wang</a>, UIUC. Summer 2022.</p>
        <p><a href="https://jeffhj.github.io/" style="color:#4133ff;">Jie Huang</a>, UIUC. Summer 2022.</p>
	    <p><a href="https://www.cse.ust.hk/~xliucr/" style="color:#4133ff;">Xin Liu</a>, HKUST. Summer 2022.</p>
	
    </div>



    
</div>
